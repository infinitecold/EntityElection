import ConfigParser
from GetVotes import GoogleVoter, WikipediaVoter
#from GoogleQuery import GoogleQuery
import MySQLdb
import random
import requests
import sys
import time
#from WikipediaQuery import WikipediaQuery

requests.packages.urllib3.disable_warnings()  # suppress warnings generated by requests package

# ugly solution to ensure unicode encoding
'''reload(sys)
sys.setdefaultencoding('utf-8')'''

# METHODS
def wikiname2freebaseID(wiki_name):
    wiki_url = "https://" + lang + ".wikipedia.org/w/?title=" + wiki_name + "&action=info"

    r = requests.get(wiki_url)
    raw_html = r.text
    start = raw_html.find('<tr id="mw-pageinfo-article-id">')  # identifier for page ID
    end = raw_html.find("</td></tr>", start)
    if lang == "es":
        wiki_id = raw_html[start+100:end]
    elif lang == "zh":
        wiki_id = raw_html[start+78:end]
    else:  # since default language is english
        wiki_id = raw_html[start+81:end]

    sys.stdout.write(wiki_name + "\t" + wiki_id)

    cur.execute("SELECT * FROM " + IDs_table + " WHERE `pageid` = " + wiki_id)
    row = cur.fetchone()
    # if no result is found in MySQL, the result is NoneType
    if row is not None:
        sys.stdout.write("\t" + row[0] + "\n")
        return row[0]
    else:
        sys.stdout.write("\n")
        return None

def collect_votes(names, freebase_ids, weighting):
    count = 0
    for name in names:
        freebase_id = wikiname2freebaseID(name)
        
        if freebase_id is not None:
            if freebase_id not in freebase_ids:
                freebase_ids[freebase_id] = 0
            score = freebase_ids[freebase_id]
            score = (10*score + 10*weighting[count])/10
            freebase_ids[freebase_id] = score
        count += 1
    return freebase_ids

# PROCESS ARGUMENTS
input_filepath = sys.argv[1] #"/eecs/research/asr/fwei/KBP/elCandidate/elCandidateProj/data/iFlyTek16/eng/edl_fuse.cluster.tsv.selected.eng"
# set language based on input file, default is english
lang = "en"
if "cmn" in input_filepath:
    lang = "zh"
elif "spa" in input_filepath:
    lang = "es"

# VARIABLES
# constants
NUM_OF_SEARCH_RESULTS = 5
GOOGLE_WEIGHTING = [2.4, 2.3, 2.2, 2.1, 2.0]
WIKIPEDIA_WEIGHTING = [1.4, 1.3, 1.2, 1.1, 1.0]

# data structures and objects
documents = {}
candidatelist = {}
google = GoogleVoter()
wikipedia = WikipediaVoter(lang)

# MySQL
config = ConfigParser.RawConfigParser()
config.read("config.ini")
db = MySQLdb.connect(host=config.get("MySQL", "host"),
                     port=int(config.get("MySQL", "port")),
                     user=config.get("MySQL", "user"),
                     passwd=config.get("MySQL", "passwd"),
                     db=config.get("MySQL", "db"))
IDs_table = "WikiID" + lang.upper()
cur = db.cursor()

# MAIN
with open(input_filepath, 'r') as input_file:
    for line in input_file:
        line_data = line.split('\t')

        document_data = line_data[3].split(':')
        document = document_data[0]
        if document not in documents:
            documents[document] = dict()
        document_offset = document_data[1]
        entity = line_data[2]
        entities = documents[document]
        entities[document_offset] = entity  # add entity to document in documents
        documents[document] = entities
        
        print(documents)
        #print("ENTITY '" + entity + "' FROM DOCUMENT '" + document + "' OF OFFSET " + document_offset)

        freebaseIDs = {}
        wiki_names = wikipedia.get_wiki_names(entity, NUM_OF_SEARCH_RESULTS)
        freebaseIDs = collect_votes(wiki_names, freebaseIDs, WIKIPEDIA_WEIGHTING)
        google_names = google.get_wiki_links(entity, NUM_OF_SEARCH_RESULTS)
        freebaseIDs = collect_votes(google_names, freebaseIDs, GOOGLE_WEIGHTING)

        candidatelist[entity] = freebaseIDs

        #del names[:]

        print(candidatelist)
        print('')
        #time.sleep(random.uniform(0, 3))  # wait a random time in seconds in this range
