from collections import OrderedDict
import ConfigParser
import io
import MySQLdb
import random
import requests
import sys
import time
from Voters import GoogleVoter, WikipediaVoter

requests.packages.urllib3.disable_warnings()  # suppress warnings generated by requests package

# ugly solution to ensure unicode encoding
'''reload(sys)
sys.setdefaultencoding('utf-8')'''


# CONSTANTS AND TUNING PARAMETERS
NUM_OF_SEARCH_RESULTS = 3  # for each voter
DECAY = 0.95
WIKIPEDIA_SCORE = 0.6
GOOGLE_SCORE = 1
WIKIPEDIA_WEIGHTING = [WIKIPEDIA_SCORE*(DECAY**n) for n in range(NUM_OF_SEARCH_RESULTS)]
GOOGLE_WEIGHTING = [GOOGLE_SCORE*(DECAY**n) for n in range(NUM_OF_SEARCH_RESULTS)]
ENTITY_DISTANCE_THRESHOLD = 100


# METHODS
def collect_votes(entity, search, freebase_ids):
    '''cur.execute("SELECT * FROM {0} WHERE `en_name` = '\"{1}\"@en'".format(name2id_table, wiki_name))
    row = cur.fetchone()
    if row is not None:
    freebase_ids = row[1].split(',')
    
    for freebase_id in freebase_ids:
    cur.execute("SELECT * FROM {0} WHERE `freebase_id` = '<http://rdf.freebase.com/ns/{1}>'".format(id2rowid_table, freebase_id))
    row = cur.fetchone()
    if row is not None:  # shouldn't be necessary
    min_row = row[1]
    max_row = row[2]
    cur.execute("SELECT * FROM {0} WHERE `row_id` > {1} AND `row_id` < {2}".format(datadump_table, min_row, max_row))
    results = cur.fetchall();
    for row in results:
    if row[1] == "<http://rdf.freebase.com/ns/common.topic.topic_equivalent_webpage>" and row[2] == "'''
    if search == "WIKIPEDIA":
        names = wikipedia.get_wiki_names(entity, NUM_OF_SEARCH_RESULTS)
        weighting = WIKIPEDIA_WEIGHTING
    elif search == "GOOGLE":
        names = google.get_wiki_names(entity, NUM_OF_SEARCH_RESULTS)
        weighting = GOOGLE_WEIGHTING
    
    print(search)

    for count, wiki_name in enumerate(names):
        if wiki_name in cache:
            freebase_id = cache[wiki_name]
            print(str(count+1) + ". " + wiki_name + '\t' + cache[wiki_name])
        else:
            # wiki name -> wiki ID
            wiki_url = "https://" + LANG + ".wikipedia.org/w/?title=" + wiki_name + "&action=info"
            r = requests.get(wiki_url)
            raw_html = r.text
            start = raw_html.find('<tr id="mw-pageinfo-article-id">')  # identifier for page ID
            end = raw_html.find("</td></tr>", start)
            if LANG == "es":
                wiki_id = raw_html[start+100:end]
            elif LANG == "zh":
                wiki_id = raw_html[start+78:end]
            else:  # since default language is english
                wiki_id = raw_html[start+81:end]

            # wiki ID -> freebase ID
            cur.execute("SELECT * FROM " + IDs_table + " WHERE `pageid` = " + wiki_id)
            row = cur.fetchone()
            if row is not None:  # if no result is found in MySQL, the result is NoneType
                freebase_id = row[0]
            else:
                freebase_id = "None"
                
            # save to cache
            cache[wiki_name] = freebase_id
            with io.open(CACHE_FILEPATH, 'a') as output_cache:
                output_cache.write(wiki_name + '\t' + freebase_id + '\n')
                    
            print(str(count+1) + ". " + wiki_name + '\t' + wiki_id + '\t' + freebase_id)

        # tabulate scores
        if freebase_id != "None":
            if freebase_id not in freebase_ids:
                freebase_ids[freebase_id] = 0
            score = freebase_ids[freebase_id]
            score = score + weighting[count]
            freebase_ids[freebase_id] = score

    return freebase_ids


# VARIABLES
# filepaths and language
if len(sys.argv) == 1 or len(sys.argv) > 4:  # makes sure the correct number of arguments were specified
    print("USAGE:\tpython [PATH TO FILE]\n\tpython [PATH TO FILE] [START INDEX]\n\tpython [PATH TO FILE] [START INDEX] [END INDEX]")
    sys.exit()

INPUT_FILEPATH = sys.argv[1] #"/eecs/research/asr/fwei/KBP/elCandidate/elCandidateProj/data/iFlyTek16/eng/edl_fuse.cluster.tsv.selected.eng"
CACHE_FILEPATH = "cache.log"

LANG = "en"  # set language based on filepath with default as english 
if "cmn" in INPUT_FILEPATH:
    LANG = "zh"
elif "spa" in INPUT_FILEPATH:
    LANG = "es"

# data structures and objects
cache = {}
documents = OrderedDict()
google = GoogleVoter()
wikipedia = WikipediaVoter(LANG)

# MySQL
config = ConfigParser.RawConfigParser()
config.read("config.ini")
db = MySQLdb.connect(host=config.get("MySQL", "host"),
                     port=int(config.get("MySQL", "port")),
                     user=config.get("MySQL", "user"),
                     passwd=config.get("MySQL", "passwd"),
                     db=config.get("MySQL", "db"))
IDs_table = "WikiID" + LANG.upper()
#name2id_table = "freebase-onlymid_-_en_name2id"
#id2rowid_table = "freebase-onlymid_-_fb-id2row-id"
#datadump_table = "freebase-onlymid_-_datadump"
cur = db.cursor()


# MAIN
with io.open(INPUT_FILEPATH, 'r', encoding='utf-8') as input_file, io.open(CACHE_FILEPATH, 'r') as input_cache:
    # read from cache
    for line in input_cache:
        line_data = line.split('\t')
        cache[line_data[0].rstrip()] = line_data[1].rstrip()
    
    # read from input file and retrieve documents
    for line in input_file:
        line_data = line.split('\t')
        document_data = line_data[3].split(':')
        document = document_data[0]
        if document not in documents:
            documents[document] = OrderedDict()
        document_offset = document_data[1]
        entity = line_data[2]
        entities = documents[document]
        entities[document_offset] = entity  # add entity to document in documents
        documents[document] = entities

# process indices
if len(sys.argv) > 3:
    MAX_INDEX = int(sys.argv[3])
else:
    MAX_INDEX = len(documents)
if len(sys.argv) > 2:
    MIN_INDEX = int(sys.argv[2])
else:
    MIN_INDEX = 0

for index, (document, info) in enumerate(documents.iteritems()):
    if index >= MIN_INDEX and index < MAX_INDEX:
        for count, (document_offset, entity) in enumerate(info.iteritems()):
            print(document + ": {" + entity + ": " + document_offset + "}")

            candidate_list = {}
            if count != 0:  # search with previous entity
                previous_indices = info.items()[count-1][0]
                previous_entity = info.items()[count-1][1]
                if int(document_offset.split('-')[0]) - int(previous_indices.split('-')[1]) < ENTITY_DISTANCE_THRESHOLD:
                    candidate_list = collect_votes(previous_entity + " " + entity, "WIKIPEDIA", candidate_list) 
                    candidate_list = collect_votes(previous_entity + " " + entity, "GOOGLE", candidate_list)
            time.sleep(random.uniform(1, 3))  # wait a random time in seconds in this range

            # search by itself
            candidate_list = collect_votes(entity, "WIKIPEDIA", candidate_list)
            candidate_list = collect_votes(entity, "GOOGLE", candidate_list)
            time.sleep(random.uniform(1, 3))  # wait a random time in seconds in this range

            if count != len(info)-1: # search with following entity
                following_indices = info.items()[count+1][0]
                following_entity = info.items()[count+1][1]
                if int(following_indices.split('-')[0]) - int(document_offset.split('-')[1]) < ENTITY_DISTANCE_THRESHOLD:
                    candidate_list = collect_votes(entity + " " + following_entity, "WIKIPEDIA", candidate_list)
                    candidate_list = collect_votes(entity + " " + following_entity, "GOOGLE", candidate_list)
            time.sleep(random.uniform(1, 3))  # wait a random time in seconds in this range

            print("CANDIDATE LIST: ")
            sorted_list = {}
            for candidate, score in sorted(candidate_list.iteritems(), key=lambda(k,v): v, reverse=True):
                sorted_list[candidate] = score
                print(candidate + ": " + "{:.3f}".format(sorted_list[candidate]))
            print('')
            
    elif index >= MAX_INDEX:
        break
