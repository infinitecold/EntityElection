import ConfigParser
from GetVotes import GoogleVoter, WikipediaVoter
#from GoogleQuery import GoogleQuery
import MySQLdb
import random
import requests
import sys
import time
#from WikipediaQuery import WikipediaQuery

'''requests.packages.urllib3.disable_warnings()  # suppress warnings generated by requests package

# ugly solution to ensure unicode encoding
reload(sys)
sys.setdefaultencoding('utf-8')'''

# METHODS
def wikiname2freebaseID(wiki_name):
    wiki_url = "https://" + lang + ".wikipedia.org/w/?title=" + wiki_name + "&action=info"

    r = requests.get(wiki_url)
    raw_html = r.text
    start = raw_html.find('<tr id="mw-pageinfo-article-id">')  # identifier for page ID
    end = raw_html.find("</td></tr>", start)
    if lang == "es":
        wiki_id = raw_html[start+100:end]
    elif lang == "zh":
        wiki_id = raw_html[start+78:end]
    else:  # since default language is english
        wiki_id = raw_html[start+81:end]

    sys.stdout.write(wiki_name + "\t" + wiki_id)

    cur.execute("SELECT * FROM " + IDs_table + " WHERE `pageid` = " + wiki_id)
    row = cur.fetchone()
    # if no result is found in MySQL, the result is NoneType
    # when a result is found, break out of the loop and use that result
    if row is not None:
        sys.stdout.write("\t" + row[0] + "\n")
        return row[0]

def collect_votes(names, freebase_ids, weighting):
    count = 0
    for name in names:
        freebase_id = wikiname2freebaseID(name)

        if freebase_id not in freebase_ids:
            freebase_ids[freebase_id] = 0
        score = freebase_ids[freebase_id]
        score = score + weighting[count]
        freebase_ids[freebase_id] = score
        count += 1

# PROCESS ARGUMENTS
input_filepath = sys.argv[1] #"/eecs/research/asr/fwei/KBP/elCandidate/elCandidateProj/data/iFlyTek16/eng/edl_fuse.cluster.tsv.selected.eng"
# get language of input file, default is english
lang = "en"
if "cmn" in input_filepath:
    lang = "zh"
elif "spa" in input_filepath:
    lang = "es"

# VARIABLES
# constants
NUM_OF_SEARCH_RESULTS = 5
GOOGLE_WEIGHTING = [2.1, 2.075, 2.05, 2.025, 2]
WIKIPEDIA_WEIGHTING = [1.1, 1.075, 1.05, 1.025, 1]

# data structures and objects
documents = {}
freebaseIDs = {}
google = GoogleVoter()
wikipedia = WikipediaVoter(lang)

# MySQL
config = ConfigParser.RawConfigParser()
config.read("config.ini")
db = MySQLdb.connect(host=config.get("MySQL", "host"),
                     port=config.get("MySQL", "port"),
                     user=config.get("MySQL", "user"),
                     passwd=config.get("MySQL", "passwd"),
                     db=config.get("MySQL", "db"))
IDs_table = "WikiID" + lang.upper()
cur = db.cursor()

# MAIN
with open(input_filepath, 'r') as input_file:
    for line in input_file:
        line_data = line.split('\t')

        document_data = line_data[3].split(':')
        document = document_data[0]
        if document not in documents:
            documents[document] = dict()
        document_offset = document_data[1]
        entity = line_data[2]
        entities = documents[document]
        entities[document_offset] = entity  # add entity to document in documents
        documents[document] = entities

        print("ENTITY '" + entity + "' FROM DOCUMENT '" + document + "' OF OFFSET " + document_offset)

        wiki_names = wikipedia.get_wiki_names(entity, NUM_OF_SEARCH_RESULTS)
        collect_votes(wiki_names, freebaseIDs, WIKIPEDIA_WEIGHTING)
        google_names = google.get_wiki_links(entity, NUM_OF_SEARCH_RESULTS)
        collect_votes(google_names, freebaseIDs, GOOGLE_WEIGHTING)

        # del names[:]

        print('')
        time.sleep(random.uniform(0, 3))  # wait a random time in seconds in this range
