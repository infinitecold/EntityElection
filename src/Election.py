import argparse
from collections import OrderedDict
import ConfigParser
import io
import logging
import MySQLdb
import random
import requests
import sys
import time
from Voters import GoogleVoter, WikipediaVoter


# CONSTANTS AND TUNING PARAMETERS
NUM_OF_SEARCH_RESULTS = 3  # for each voter
DECAY = 0.95
WIKIPEDIA_FACTOR = 0.75  # multiplication factor to all values depending on search method
GOOGLE_FACTOR = 1.00
ADJACENT_FACTOR = 0.50
INDIVIDUAL_FACTOR = 1.00
ENTITY_DISTANCE_THRESHOLD = 60  # distance between entity offsets in order to put together
NIL_THRESHOLD = 2.00


# INITIALIZATION
# suppresses warnings generated by requests package
requests.packages.urllib3.disable_warnings()

# ensures unicode encoding
reload(sys)
sys.setdefaultencoding("utf-8")

# initializes the logger
logging.basicConfig(format='%(asctime)s\t\t%(message)s', level=logging.INFO)
logging.getLogger("requests").setLevel(logging.WARNING)

# parses command-line arguments
parser = argparse.ArgumentParser()
parser.add_argument('inputfilepath', type=str, help='filepath to .txt file to be processed')  #"/eecs/research/asr/fwei/KBP/elCandidate/elCandidateProj/data/iFlyTek16/eng/edl_fuse.cluster.tsv.selected.eng"
parser.add_argument('--outputfilepath', type=str, default="../data/results/out.txt", help='filepath to output .txt file')
parser.add_argument('--cachefilepath', type=str, default="../data/cache/Election.cache", help='filepath to .cache file')
parser.add_argument('--lang', type=str, default="en", help='language of input file')
parser.add_argument('--startindex', type=int, default=0, help='index of first document in file to be processed')
parser.add_argument('--endindex', type=int, default=sys.maxsize, help='index of last document in file to be processed') 
args = parser.parse_args()


# VARIABLES
TEAM_NAME = "iNCML0"

# data structures and objects
cache = {}
documents = OrderedDict()
google = GoogleVoter()
wikipedia = WikipediaVoter(args.lang)

# MySQL
config = ConfigParser.RawConfigParser()
config.read("../config.ini")
db = MySQLdb.connect(host=config.get("MySQL", "host"),
                     port=int(config.get("MySQL", "port")),
                     user=config.get("MySQL", "user"),
                     passwd=config.get("MySQL", "passwd"),
                     db=config.get("MySQL", "db"))
IDs_table = "WikiID" + args.lang.upper()
cur = db.cursor()


# METHODS
def collect_votes(entity, search, freebase_ids):
    # entity -> wiki names (via Wikipedia or Google)
    if search[0] == "WIKIPEDIA":
        names = wikipedia.get_wiki_names(entity, NUM_OF_SEARCH_RESULTS)
        if search[1] == "ADJACENT":
            weighting = [(DECAY**n)*WIKIPEDIA_FACTOR*ADJACENT_FACTOR for n in range(NUM_OF_SEARCH_RESULTS)]
        else:
            weighting = [(DECAY**n)*WIKIPEDIA_FACTOR*INDIVIDUAL_FACTOR for n in range(NUM_OF_SEARCH_RESULTS)]
    elif search[0] == "GOOGLE":
        names = google.get_wiki_names(entity, NUM_OF_SEARCH_RESULTS)
        if search[1] == "ADJACENT":
            weighting = [(DECAY**n)*GOOGLE_FACTOR*ADJACENT_FACTOR for n in range(NUM_OF_SEARCH_RESULTS)]
        else:
            weighting = [(DECAY**n)*GOOGLE_FACTOR*INDIVIDUAL_FACTOR for n in range(NUM_OF_SEARCH_RESULTS)]

    for count, wiki_name in enumerate(names):
        # if wiki name found in cache, wiki name -> freebase ID via cache
        if wiki_name in cache:
            freebase_id = cache[wiki_name]
            logging.info("{0} {1}. {2}\t{3}".format(search[0], count+1, wiki_name, cache[wiki_name]))
        else:
            # wiki name -> wiki ID
            wiki_url = "https://" + args.lang + ".wikipedia.org/w/?title=" + wiki_name + "&action=info"
            r = requests.get(wiki_url)
            raw_html = r.text
            start = raw_html.find('<tr id="mw-pageinfo-article-id">')  # identifier for page ID
            end = raw_html.find("</td></tr>", start)
            if args.lang == "es":
                wiki_id = raw_html[start+100:end]
            elif args.lang == "zh":
                wiki_id = raw_html[start+78:end]
            else:  # since default language is english
                wiki_id = raw_html[start+81:end]

            # wiki ID -> freebase ID
            cur.execute("SELECT * FROM " + IDs_table + " WHERE `pageid` = " + wiki_id)
            row = cur.fetchone()
            if row is not None:  # if no result is found in MySQL, the result is NoneType
                freebase_id = row[0]
            else:
                freebase_id = "None"
                
            # save to cache
            cache[wiki_name] = freebase_id
            with io.open(args.cachefilepath, 'a') as output_cache:
                output_cache.write(wiki_name + '\t' + freebase_id + '\n')
            
            logging.info("{0} {1}. {2}\t{3}\t{4}".format(search[0], count+1, wiki_name, wiki_id, freebase_id))

        # tabulate scores
        if freebase_id != "None":
            if freebase_id not in freebase_ids:
                freebase_ids[freebase_id] = 0
            score = freebase_ids[freebase_id]
            score = score + weighting[count]
            freebase_ids[freebase_id] = score

    return freebase_ids


# MAIN
# read from files
with io.open(args.inputfilepath, 'r', encoding='utf-8') as input_file, io.open(args.cachefilepath, 'r') as input_cache:
    # read from cache file
    for line in input_cache:
        line_data = line.split('\t')
        cache[line_data[0].rstrip()] = line_data[1].rstrip()
    
    # read from input file and retrieve all documents
    for line in input_file:
        line_data = line.split('\t')
        document_data = line_data[3].split(':')
        document = document_data[0]
        if document not in documents:
            documents[document] = OrderedDict()
        document_offset = document_data[1]
        entity = (line_data[2], line_data[5], line_data[6])  # (name, entity type (ORG, PER, etc.), entity type (NAM/NOM))
        entities = documents[document]  # retrieve existing entities associated with the document
        entities[document_offset] = entity  # add current entity
        documents[document] = entities

for index, (document, info) in enumerate(documents.iteritems()):
    if index >= args.startindex and index < args.endindex:
        nil = {}  # saves NIL entities for the current document
        for count, (document_offset, entity_info) in enumerate(info.iteritems()):
            entity = entity_info[0]
            candidate_list = {}
            searches = 0
            logging.info("DOCUMENT {0}: {1} ({2})".format(document, entity, document_offset))
            
            # search with previous entity
            if count != 0:
                previous_indices = info.items()[count-1][0]
                previous_entity = info.items()[count-1][1][0]
                if int(document_offset.split('-')[0]) - int(previous_indices.split('-')[1]) < ENTITY_DISTANCE_THRESHOLD:
                    with_previous = previous_entity + " " + entity
                    logging.info("SEARCH TERM: '{0}':".format(with_previous))
                    candidate_list = collect_votes(with_previous, ("WIKIPEDIA", "ADJACENT"), candidate_list) 
                    candidate_list = collect_votes(with_previous, ("GOOGLE", "ADJACENT"), candidate_list)
                    searches += 1
            time.sleep(random.uniform(3, 5))  # wait a random time in seconds in this range

            # search by itself
            logging.info("SEARCH TERM: '{0}':".format(entity))
            candidate_list = collect_votes(entity, ("WIKIPEDIA", "INDIVIDUAL"), candidate_list)
            candidate_list = collect_votes(entity, ("GOOGLE", "INDIVIDUAL"), candidate_list)
            searches += 1
            time.sleep(random.uniform(3, 5))  # wait a random time in seconds in this range

            # search with following entity
            if count != len(info)-1:
                following_indices = info.items()[count+1][0]
                following_entity = info.items()[count+1][1][0]
                if int(following_indices.split('-')[0]) - int(document_offset.split('-')[1]) < ENTITY_DISTANCE_THRESHOLD:
                    with_following = entity + " " + following_entity
                    logging.info("SEARCH TERM: '{0}':".format(with_following))
                    candidate_list = collect_votes(with_following, ("WIKIPEDIA", "ADJACENT"), candidate_list)
                    candidate_list = collect_votes(with_following, ("GOOGLE", "ADJACENT"), candidate_list)
                    searches += 1
            time.sleep(random.uniform(3, 5))  # wait a random time in seconds in this range
            
            # choose best candidate/NIL
            sorted_list = []
            final_answer = "NIL"
            if len(candidate_list) > 0:
                for candidate, score in sorted(candidate_list.iteritems(), key=lambda(k,v): (v,k), reverse=True):
                    sorted_list.append((candidate, round(score*(3/searches), 4)))  # normalize scores based on number of searches
                if sorted_list[0][1] > NIL_THRESHOLD:
                    final_answer = sorted_list[0][0]
                else:
                    if entity not in nil:
                        final_answer = final_answer + str(len(nil))
                        nil[entity] = final_answer
                    else:
                        final_answer = nil[entity]
            else:
                if entity not in nil:
                        final_answer = final_answer + str(len(nil))
                        nil[entity] = final_answer
                else:
                    final_answer = nil[entity]
            logging.info("FINAL ANSWER: {0}".format(final_answer))
            logging.info("CANDIDATE LIST: {0}\n".format(sorted_list))
            
            with io.open(args.outputfilepath, 'ab') as output_file:
                output_file.write("{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\t{7}\n".format(TEAM_NAME, "TRAINING",
                                                                                  info.items()[count][1][0],
                                                                                  document + ":" + info.items()[count][0],
                                                                                  final_answer,
                                                                                  info.items()[count][1][1],
                                                                                  info.items()[count][1][2], "1.0"))
            '''logging.info("{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\t{7}".format(TEAM_NAME, "TRAINING",
                                                                                  info.items()[count][1][0],
                                                                                  document + ":" + info.items()[count][0],
                                                                                  final_answer,
                                                                                  info.items()[count][1][1],
                                                                                  info.items()[count][1][2], "1.0"))'''
    elif index >= args.endindex:
        break
